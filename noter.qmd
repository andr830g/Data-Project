---
title: "Dataprojekt noter"
author: "Lucas"
format: pdf
---

## Uge 1 | Overblik samt præsentation af projekter

### Lektion 1

Underviser er *Asger*.


$$
\text{data }(x_i,u_i)
$$

x er antal lykkes og u er straffeforsøg

$$
\text{plot } (n_i, \frac{x_i}{u_i})
$$

$$
x_i \sim Bin(n_i, P_i)
$$

Model

$$
\hat{P_i} = \frac{x_i}{n_i}
$$


middelværdi

$$
E(x_i) = u_ip_i 
$$



Varians

$$
var(x_i)= n_ip_i(1-p_i)
$$

Lær de gænske fordelinger

Binomial,

tæthed,
middel
og varians

$$
var(\hat{P_i})=var(\frac{x_i}{n_i}) \\
\frac{1}{n_i^2}var(x_i) \\
\frac{p_i(1-p_i)}{n_i}
$$


Det fortæller os noget om usikkerheden. 

I papir tilføjes en kvalifikationstærskel. Det gør at for ESPN kan opgøre
hvilke spiller der er bedst. 

Udregning af P værdi til at rangerer.

Beregn H0 og H1

$$

$$


Central limit theorem siger at vi kan for binomial fordeling til at være approx Normalt fordelt. 

$$
N(n_ip_i,n_1p_i*(1-p_i))
$$
t = test statistik


$$
t_i=\frac{x_i-n_ip_i}{\sqrt{ni_pi(1-p_i)}} \approx N(0,1)
$$

Gælder under H0.

Gang med ni

$$
n_i\frac{\hat{P_i}-p_0}{\sqrt{u_ip_o(1-p_0)}} \in\sqrt{n_i}(\hat{P_i}-P_0)
$$

Den sidste del er effekt størrelse, men det sqrt er sample size. 

P værdien på enheder med stor varians skal man passe på. 

a og b fra beta fordeling kender vi ikke, men vi kan ud fra vores viden give den en værdi. Bruger et udtryk. 



### Lektion 2

Radiotherapy, label tools. 
autosegmentation using deep learning

ELAISA projects `The potential of E Learning interventions for AI assisted contouring Skills in rAdiotheray`

hoved hals kræft, 
Parotid
submandibular gland
oral cavity
mandible


Primary score, DICE score. 

Secondary
Haussodrf ditance, surface dice.

Variational autoencoder (VAE)


## Uge 2

### Lektion 1

## Uge  3

### opgave

```{r}
set.seed(1234)
N <- 1000
theta <- rgamma(N, 2, 0.4)
hist(theta)
```

```{r}
rgamma(N, )
```


$$
E(\theta|y)=\frac{A}{B}=\frac{a+1}{y+b}
$$

$$
\frac{a}{y+b}+\frac{1}{y+b}
$$

Middelværdi af prior

$$
\hat_{prior} \theta=(\theta)=\frac{a}{b}
$$



$$
=\frac{a}{b}\frac{b}{by+b}+ \frac{1}{y}\frac{y}{y+b}
$$

Normal Normal model conjugate bias

```{r}
y = 0.082 # alkohol procent
gr <- 0.08
# test udstyr std på 0.01
sig <- 0.01
```


mu = sande alkoholprocent

model

$$
y|\mu \sim N(\mu,\sigma^2)
$$

Læg en prior.
modeller usikkerhed

'prior'
$$
\mu_0 = 0.05 \\
\frac{\sigma^2}{m}=0.02^2 = 4 \sigma^2
$$

dvs. m=1/4
prior
$$
\mu \sim N(\mu_0,\frac{\sigma^2}{m})
$$

Posterior

$$
\mu|y\sim N(A,B)
$$
A=m/m+1 mu_0 + 1/m+1 y

shrinkige er det som sker i ridge og lasso og det har en fordel at variansen
bliver stabiliseret. Det er noget som er unikt for bayes og en klar fordel
ved dette valg frem for freq. 


```{r}
set.seed(1234)
N <- 1000
theta <- rgamma(N, 2, 0.4)
hist(theta)

```

```{r}
nor <- rnorm(N,0.05, (1/4))
hist(nor)
```

```{r}
d <- density(nor) # returns the density data
plot(d) 
```






