---
title: "Forecasting of volatile Danish Electricity Prices"
author: 'Andreas Skiby Andersen, Andreas Hyldegaard Hansen'
date: 'Last update: `r Sys.Date()`'
output:
  html_document:
    theme: united
    code_folding: show
    number_sections: true
    toc: yes
    toc_float: true
    toc_depth: 5
  pdf_document:
    number_sections: true
    toc: yes
    toc_depth: '5'
---

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(tseries)
library(lubridate)
library(Kendall)
library(kableExtra)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
prices <- read.csv("data/price_clean.csv") %>%
  mutate(
         hour_utc = lubridate::as_datetime(hour_utc),
         hour_dk = lubridate::as_datetime(hour_dk),
         spot_price_dkk = as.numeric(spot_price_dkk))
con <- read.csv("data/con_clean.csv")
prod <- read.csv("data/prod_clean.csv")
```

```{r message=FALSE, warning=FALSE, include=FALSE}
prices <- prices %>%
  arrange(hour_utc) %>%
  filter(price_area == "DK1")

con <- con %>%
  arrange(hour_utc)

prod <- prod %>%
  filter(price_area == "DK1") %>%
  arrange(hour_utc)

prices["Hour"] <- hour(prices$hour_utc)
prices["Wday"] <- wday(prices$hour_utc, label=T)
prices["Week"] <- week(prices$hour_utc)
prices["Month"] <- month(prices$hour_utc)
prices["Year"] <- year(prices$hour_utc)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
prices <- prices %>%
  filter(price_area == "DK1")
```


# Introduction

Great focus has been cast on the commodity sector during the period 2021 to present day, as the danish society has seen large volatility in electricity and gas prices. In the period, the electricity spot price has been affected by the lack of wind and worldwide consequences of inflation, freight chaos and the Russian-Ukraine war causing record high prices for the consumers.

```{r echo=FALSE, message=FALSE, warning=FALSE}
plot(prices$spot_price_dkk, type="l", main="Spot price in DK1", xlab="", ylab="DKK per MWh", xaxt="n")
axis(side=1, 
     at=c(1, 1682, 6702, 10730, 13404, 18842, 20106, 24098, 26808), 
     labels=c("2020-01-01", "Lockdown", "2020-10-06", "Evergreen","2021-07-12", "Invasion", "2022-04-17", "CPI peak", "2023-01-21"), 
     las=2)
abline(v=1682, col="red", lty=2, lwd=2)
abline(v=10730, col="red", lty=2, lwd=2)
abline(v=18842, col="red", lty=2, lwd=2)
abline(v=24098, col="red", lty=2, lwd=2)
```

Above, electricity spot prices in DK1 are shown for the period 2020-01-01 to 2023-01-21. Red lines indicate the danish lockdown due to COVID-19 2020-03-11, the Evergreen blockade of the Suez Canal 2021-03-23, the Russian invasion of Ukraine 2022-02-24 and the danish peak inflation of 10.1% in October 2022.

This project is focusing exclusively in spot prices in price area DK1 which is Denmark east of the Great belt and the project will analyze time series data of electricity spot prices to find ARIMA models which are better than a naive benchmark model.

The data consists of three datasets. The first contains electricity spot prices per hour in DKK pr. MWh over the period 2020-01-01 hour 0 (midnight) to 2023-01-31 hour 23 danish time. The price areas are Denmark east of the Great Belt (DK1), Denmark west of the great belt (DK2), Sweden southern part (SE4), Sweden area sorrounding Stockholm (SE3), Norway southern part (NO2), Germany (DE).

The second contains data of electricity excahnge between Denmark and neighboring countries and danish production of electricity in MWh from fossile fuels (gas, oil, coal) and sustainable energy (biomass, offshore wind, onshore wind, hydro) and burning of waste per hour in the same time period.

The third contains data on consumption in kWh from different different grid companies per hour in the period 2020-01-01 hour 0 to 2023-01-21 hour 23.


# Data cleaning

In this section, we are referring to our work done in data_cleaning.qmd.

We have gathered our raw data from www.energidataservice.dk, where our data was in a .JSON file format.

After reading the data, we made sure that all our variables were read as the correct datatypes, and transformed all missing values into NA values. In the "Production" data set, we also dropped variables from the data set, that had too many missing values to be able to use them. In the "Consumption" data set, we only had 3 variables, but still a lot of missing values. To keep as much information from the "Consumption" data set, we then transformed all the missing values to 0's.

After having cleaned the columns, we then moved on to the rows, where we found out that the start and end times of our 3 data sets did not match. All the data sets have the same first observation regarding time, but the "Consumption" data set, were missing some observations at the end. Hence, we had to cut off data from the other two data sets, to make them all equal length.

We then had a strange problem where there was 3 somewhat random observations that were missing from the "Production" data set. We then had to synthetically add the previous observation's values to the missing rows to fill in the missing observations.
We are aware that modification of 3 out of 26808 observations can have an impact on results but we assume it's insignificant. Additionally due to the nature of time series, the observations must be present in the data for the models to work.

At last, we also divided the columns in the "Consumption" data set with 1000, since we found out that the electricity used in the "Consumption" data set was in kwh, where we had electricity in Mwh in the other data sets.

Having the data in a .JSON file format made reading the data very slow, so we decided to save the cleaned data as a .csv file format, which made it much faster to read.

We then followed the same process to create cleaned test data to measure our model's performance at the end of the report.


# Trend

In this section, we are referring to our work done in Detrending.Rmd.

The first thing we want to look at when dealing with time series data, is to check whether or not our time series is stationary. This is difficult to check, so instead we will be checking whether or not our time series data is covariance stationary, which can be done with an ADF (Augmented Dickey-Fuller) test, a KPSS (Kwiatkowski-Phillips-Schmidt-Shin) or by observing the ACF (Auto-Correlation Function) plot.

We first check if the raw data is covariance stationary.

```{r}
acf(prices$spot_price_dkk, lag.max = 1000)
```

```{r message=FALSE, warning=FALSE}
adf.test(prices$spot_price_dkk)
kpss.test(prices$spot_price_dkk)
```

We observe that the two tests don't agree. The null hypothesis for the ADF test is "Not stationary", while the null hypothesis for the KPSS test is "Stationary".
When observing the ACF plot, we can clearly see that there is high autocorrelation in the data. Hence, we trust the KPSS test here and conclude that the time series is not covariance stationary.

Next step is then to try to transform the data so that it becomes stationary, since this is a necessity to assure the correctness of the models we will be using later on (ARIMA/SARIMA).

To transform the data to become stationary, there are multiple possible methods to use. 

We could for example try to take the logarithm on the data, but this does not work in our case, since we in some cases have negative values. This can be seen in the data_transformation.Rmd file.

A different approach is to first difference the data, so that every observation is subtracted from the previous observation.

At last, we can try to detrend the data, by fitting a regression to the data to mimic the underlying trend, which we afterwards can subtract from the data to remove the fitted trend.

We first try to remove the underlying trend with a linear regression.

```{r}
prices$t <- sequence(26808, 1)

# Function for calculating sum of squared error
sse <- function(data, par) {
  with(data, sum((trendfn(data, par) - spot_price_dkk)^2))
}
```

```{r message=FALSE, warning=FALSE}
trendfn <- function(data, par) {
  with(data, par[1] + par[2]*t)
}

optimum <- optim(c(1, 2), sse, data=prices)
par_result <- optimum[[1]]
optimum

prices_trend <- trendfn(prices, par_result)
residuals_trend <- prices$spot_price_dkk - prices_trend

plot(prices$spot_price_dkk, main="Trend function", col=alpha("black", 0.5), type="l")
lines(prices$t, prices_trend, col="red")

plot(prices$t, residuals_trend, main="Residuals/detrend", col=alpha("black", 0.5), type="l")
print(sqrt(mean(residuals_trend^2)))
```

We observe that a linear trend looks to be good at catching the trend in the somewhat "stable" first part of the data, but gets worse at catching the trend in the "volatile" last part of the data.

```{r}
acf(residuals_trend, lag.max=1000)
```

We then evaluate the detrending against using first differencing using ADF test and KPSS test.

```{r message=FALSE, warning=FALSE}
adf.test(residuals_trend)
kpss.test(residuals_trend)
adf.test(diff(prices$spot_price_dkk, 1))
kpss.test(diff(prices$spot_price_dkk, 1))
```

Looking at the KPSS test, the linear trend does not look to have made much of a difference regarding covariance stationarity, but the KPSS test says that the first differenced data is stationary.

We then tried different variants of sine functions, offset sine functions + linear trends and sums of offset since functions + linear trends, to try to find a good fit of the data, where the best underlying function we found was the following sum of 7 offset sine functions + linear trends.

```{r message=FALSE, warning=FALSE}
trendfn <- function(data, par) {
  with(data, par[1] 
       + par[2]*sin((t+par[3])/240) 
       + par[4]*sin((t+par[5])/480)
       + par[6]*sin((t+par[7])/720)
       + par[8]*sin((t+par[9])/960)
       + par[10]*sin((t+par[11])/1200)
       + par[12]*sin((t+par[13])/1440)
       + par[12]*sin((t+par[13])/1680))
}

subset_prices <- prices %>%
  filter(t >= 15000)
optimum <- optim(c(10000, 
                   5000, 0, 
                   2000, 24,
                   1000, 48,
                   100, 72,
                   100, 96,
                   100, 120,
                   1000, 144), sse, data=subset_prices)
par_result <- optimum[[1]]
optimum

prices_trend <- trendfn(subset_prices, par_result)
residuals_trend <- subset_prices$spot_price_dkk - prices_trend

plot(subset_prices$t, subset_prices$spot_price_dkk, main="Trend function", col=alpha("black", 0.5), type="l")
lines(subset_prices$t, prices_trend, col="red")

plot(subset_prices$t, residuals_trend, main="Residuals/detrend", col=alpha("black", 0.5), type="l")
print(sqrt(mean(residuals_trend^2)))
```

We note that we are here just trying to fit a trend on the "volatile" last part of the data. We then see that it looks like we have fitted the underlying function quite well, but we will have to check what the KPSS test says regarding stationarity.

```{r}
acf(residuals_trend, lag.max=1000)
```

```{r message=FALSE, warning=FALSE}
adf.test(residuals_trend)
kpss.test(residuals_trend)
adf.test(diff(subset_prices$spot_price_dkk, 1))
kpss.test(diff(subset_prices$spot_price_dkk, 1))
```

Looking at the KPSS test, it now says that the time series is almost covariance stationary, but the ACF plot still does not look too good. It is still also significantly less covariance stationary, than just using first differencing.

We also tried using a nonparametric method to fit the underlying trend in the data, but this is of course not a viable solution, since it is nonparametric and will of course then not work for forecasting. One thing, we did discover from the nonparametric model, was that when we gave it a span closer to 0, the detrending with the nonparametric model, came closer and closer to a first difference.

```{r}
prices_loess <- loess(spot_price_dkk~t, data = prices, span=0.01)
prices_trend <- predict(prices_loess, prices$t)

plot(prices$t, prices$spot_price_dkk, main="Nonparametric fit", type='l')
lines(prices$t, prices_trend, col='red', lwd=2)

plot(prices$t, prices$spot_price_dkk - prices_trend, main="Residuals/detrend", type='l')
```

```{r}
plot(diff(prices$spot_price_dkk, 1), main="First Differenced", type="l")
```

This made us decide on using first differencing to transform our data to become covariance stationary.

# Seasonality

Time series data often experiences some sort of seasonality caused by varying factors over specific time periods. To "catch" possible seasonality when creating models for forecasting, we want to test in what time periods the data experiences seasonality.

We start by checking for hourly and daily seasonality by plotting the mean prices grouped by hour and weekday. We then see if we can find a recurring predictable pattern.

```{r message=FALSE, warning=FALSE}
prices %>%
  group_by(Hour, Wday) %>%
  summarize(hour=Hour, day=Wday, week=Week, month=Month, year=Year, mean_price=mean(spot_price_dkk)) %>%
  ggplot(aes(x=hour, 
             y=mean_price, 
             group=factor(day), 
             color=factor(day))) + 
  geom_line()
```

When looking at the plot, it is very clear that the prices are at their highest at around 6 AM and around 5 PM as well as the prices being at their lowest at around 2 AM and around 12 PM. This pattern seems recurring and predictable. Hence, we will note that the data experiences hourly seasonality.

For daily seasonality, it looks as if there is a recurring pattern of the prices being at their highest on mondays and tuesdays and the prices being at their lowest on saturdays and sundays. We will check this over again by now checking for daily and weekly/monthly seasonality by plotting the mean prices grouped by weekday and month.

```{r message=FALSE, warning=FALSE}
prices %>%
  group_by(Wday, Month) %>%
  summarize(hour=Hour, day=Wday, week=Week, month=Month, year=Year, mean_price=mean(spot_price_dkk)) %>%
  ggplot(aes(x=day, 
             y=mean_price, 
             group=factor(Month), 
             color=factor(Month))) + 
  geom_line()
```

When looking at this plot, we again see the same pattern regarding daily seasonality. Hence, we will note that the data experiences daily seasonality.

For weekly/monthly seasonality, it looks like there is a recurring pattern of the prices being at their highest in the summer months and the prices being at their lowest in the winter months, with the exception of December. We will again check this one more time by now plotting the mean prices grouped by month and weekday.

```{r message=FALSE, warning=FALSE}
prices %>%
  group_by(Wday, Month) %>%
  summarize(hour=Hour, day=Wday, week=Week, month=Month, year=Year, mean_price=mean(spot_price_dkk)) %>%
  ggplot(aes(x=month, 
             y=mean_price, 
             group=factor(day), 
             color=factor(day))) + 
  geom_line()
```

This gives us a much clearer view of the weekly/monthly seasonality, and we can confirm what we saw on the previous plot. Hence, the data experiences weekly/monthly seasonality.

# Forecasting

# Results

```{r include=FALSE}
Result_table <- data.frame(model = character(), free_arima = character(), seasonal = character(), free_seasonal = character(), xreg = character(), RMSE = double(), stringsAsFactors = FALSE)
```

```{r echo=FALSE}
Result_table <- rbind(Result_table, list("Linear Regression", "", "", "", "", 499.7))
Result_table <- rbind(Result_table, list("Sinusoidal Regression", "", "", "", "", 847.4))
Result_table <- rbind(Result_table, list("Sum of Sinusoidal Regressions", "", "", "", "", 1535.0))
Result_table <- rbind(Result_table, list("ARIMA(1,1,0)", "", "", "", "", 431.5))
Result_table <- rbind(Result_table, list("ARIMA(24,1,0)", "24th", "", "", "", 417.5))
Result_table <- rbind(Result_table, list("ARIMA(24,1,0)", "", "", "", "", 371.8))
Result_table <- rbind(Result_table, list("SARIMA(24,1,0)", "24th", "(7,0,0)", "1st, 2nd, 7th", "", 380.9))
Result_table <- rbind(Result_table, list("SARIMA(24,1,0)", "", "(7,0,0)", "", "", 362.7))
Result_table <- rbind(Result_table, list("SARIMA(1,1,0)", "", "(7,0,0)", "1st, 2nd, 7th", "", 388.8))
Result_table <- rbind(Result_table, list("SARIMA(1,1,0)", "", "(7,0,0)", "", "", 368.0))
Result_table <- rbind(Result_table, list("SARIMA(24,1,0)", "", "(7,1,0)", "", "", 363.7))
Result_table <- rbind(Result_table, list("SARIMA(24,1,1)", "", "(7,0,0)", "", "", 369.2))
Result_table <- rbind(Result_table, list("SARIMA(24,1,1)", "24th", "(7,0,0)", "", "", 362.5))
Result_table <- rbind(Result_table, list("SARIMA(24,1,1)", "24th", "(7,0,0)", "1st, 2nd, 7th", "", 381.2))
Result_table <- rbind(Result_table, list("SARIMA(24,1,1)", "", "(7,0,1)", "", "", 344.0))
Result_table <- rbind(Result_table, list("SARIMA(24,1,1)", "24th", "(7,0,1)", "", "", 362.0))
Result_table <- rbind(Result_table, list("SARIMA(24,1,1)", "", "(7,0,1)", "1st, 2nd, 7th", "", 356.7))
Result_table <- rbind(Result_table, list("SARIMA(24,1,1)", "24th", "(7,0,1)", "1st, 2nd, 7th", "", 356.7))
Result_table <- rbind(Result_table, list("SARIMA(24,1,24)", "", "(7,0,0)", "", "", 346.6))
Result_table <- rbind(Result_table, list("SARIMA(24,1,24)", "", "(7,0,7)", "", "", 361.2))
Result_table <- rbind(Result_table, list("ARIMA(24,1,0)", "", "", "", "Weekend", 366.3))
Result_table <- rbind(Result_table, list("SARIMA(24,1,1)", "", "(7,0,1)", "", "Weekend", 341.2))
Result_table <- rbind(Result_table, list("ARIMA(24,1,0)", "", "", "", "Weekdays", 365.0))
Result_table <- rbind(Result_table, list("ARIMA(24,1,0)", "", "", "", "Weekend, Season", 366.4))
Result_table <- rbind(Result_table, list("ARIMA(24,1,0)", "", "", "", "Weekend, Wind", 316.8))
Result_table <- rbind(Result_table, list("ARIMA(24,1,0)", "", "", "", "Weekend, Wind, Solar", 318.6))
Result_table <- rbind(Result_table, list("ARIMA(24,1,0)", "", "", "", "Weekend, FossilProduce", 347.7))
Result_table <- rbind(Result_table, list("ARIMA(24,1,0)", "", "", "", "Weekend, Wind, FossilProduce", 305.8))
Result_table <- rbind(Result_table, list("ARIMA(24,1,0)", "", "", "", "Weekend, Wind, FossilProduce, Consumption", 305.2))
Result_table <- rbind(Result_table, list("ARIMA(24,1,0)", "", "", "", "Weekend, Wind, FossilProduce, Biomass", 302.4))
Result_table <- rbind(Result_table, list("ARIMA(24,1,0)", "", "", "", "Weekend, Wind, FossilProduce, Biomass, Waste", 299.4))
Result_table <- rbind(Result_table, list("ARIMA(24,1,0)", "", "", "", "Weekend, Wind, FossilProduce, Biomass, Waste, Exchange", 276.1))
Result_table <- rbind(Result_table, list("ARIMA(1,1,0)", "", "", "", "Weekend, Wind, FossilProduce, Biomass, Waste, Exchange", 303.5))
Result_table <- rbind(Result_table, list("SARIMA(24,1,0)", "24th", "(7,0,0)", "1st, 2nd, 7th", "Weekend, Wind, FossilProduce, Biomass, Waste, Exchange", 311.8))
Result_table <- rbind(Result_table, list("SARIMA(24,1,1)", "", "(7,0,1)", "", "Weekend, Wind, FossilProduce, Biomass, Waste, Exchange", 263.9))
colnames(Result_table) <- c("model", "free_arima", "seasonal", "free_seasonal", "xreg", "RMSE")
kbl(Result_table) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "bordered"), full_width = F, position = "left", font_size = 10)
```

```{r include=FALSE}
CV_table <- data.frame(model = character(), free_arima = character(), seasonal = character(), free_seasonal = character(), xreg = character(), RMSE_subset_1 = double(), RMSE_subset_2 = double(), RMSE_subset_3 = double(), RMSE_subset_4 = double(), mean_RMSE = double(), sd_RMSE = double(), stringsAsFactors = FALSE)
```

```{r echo=FALSE}
CV_table <- rbind(CV_table, list("ARIMA(24,1,0)", "", "", "", "", 441.3, 704.8, 442.1, 371.0, 489.8, 147.2))
CV_table <- rbind(CV_table, list("SARIMA(24,1,1)", "", "(7,0,1)", "", "", 393.3, 661.9, 391.7, 342.1, 447.3, 145.1))
CV_table <- rbind(CV_table, list("ARIMA(24,1,0)", "", "", "", "Weekend, Wind", 354.6, 674.8, 405.2, 364.3, 449.7, 151.6))
CV_table <- rbind(CV_table, list("ARIMA(24,1,0)", "", "", "", "Weekend, Wind, FossilProduce, Biomass, Waste, Exchange", 240.9, 543.2, 328.5, 364.3, 369.2, 127.1))
CV_table <- rbind(CV_table, list("SARIMA(24,1,1)", "", "(7,0,1)", "", "Weekend, Wind, FossilProduce, Biomass, Waste, Exchange", 233.0, 539.5, 316.7, 337.7, 356.7, 130.0))
CV_table <- rbind(CV_table, list("ARIMA(23,1,0)", "", "", "", "Weekend, Wind, FossilProduce, Biomass, Waste, Exchange", 240.5, 533.7, 345.2, 359.4, 369.7, 121.5))
colnames(CV_table) <- c("model", "free_arima", "seasonal", "free_seasonal", "xreg", "sub_RMSE_1", "sub_RMSE_2", "sub_RMSE_3", "sub_RMSE_4", "mean_RMSE", "sd_RMSE")
kbl(CV_table) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "bordered"), full_width = F, position = "left", font_size = 10)
```

