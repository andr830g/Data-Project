---
title: "Forecasting of volatile Danish Electricity Prices"
author: 'Andreas Skiby Andersen, Andreas Hyldegaard Hansen'
date: 'Last update: `r Sys.Date()`'
output:
  html_document:
    theme: united
    code_folding: show
    number_sections: true
    toc: yes
    toc_float: true
    toc_depth: 5
  pdf_document:
    number_sections: true
    toc: yes
    toc_depth: '5'
---

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(tseries)
library(forecast)
library(lubridate)
library(Kendall)
library(kableExtra)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
prices <- read.csv("data/price_clean.csv") %>%
  mutate(
         hour_utc = lubridate::as_datetime(hour_utc),
         hour_dk = lubridate::as_datetime(hour_dk),
         spot_price_dkk = as.numeric(spot_price_dkk))
con <- read.csv("data/con_clean.csv")
prod <- read.csv("data/prod_clean.csv")
```

```{r message=FALSE, warning=FALSE, include=FALSE}
prices <- prices %>%
  arrange(hour_utc) %>%
  filter(price_area == "DK1")

con <- con %>%
  arrange(hour_utc)

prod <- prod %>%
  filter(price_area == "DK1") %>%
  arrange(hour_utc)

prices["Hour"] <- hour(prices$hour_utc)
prices["Wday"] <- wday(prices$hour_utc, label=T)
prices["Week"] <- week(prices$hour_utc)
prices["Month"] <- month(prices$hour_utc)
prices["Year"] <- year(prices$hour_utc)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
prices <- prices %>%
  filter(price_area == "DK1")
```

# Introduction

Great focus has been cast on the commodity sector during the period 2021 to present day, as the danish society has seen a great rise and large volatility in electricity prices and gas prices. In the period, the electricity spot price has been affected by worldwide consequences of inflation, freight chaos and the Russian-Ukraine war, causing record high prices for the consumers.

```{r echo=FALSE, message=FALSE, warning=FALSE}
plot(prices$spot_price_dkk, type="l", main="Spot price in DK1", xlab="", ylab="DKK per MWh", xaxt="n")
axis(side=1, 
     at=c(1, 1682, 6702, 10730, 13404, 18842, 20106, 24098, 26808), 
     labels=c("2020-01-01", "Lockdown", "2020-10-06", "Evergreen","2021-07-12", "Invasion", "2022-04-17", "CPI peak", "2023-01-21"), 
     las=2)
abline(v=1682, col="red", lty=2, lwd=2)
abline(v=10730, col="red", lty=2, lwd=2)
abline(v=18842, col="red", lty=2, lwd=2)
abline(v=24098, col="red", lty=2, lwd=2)
```

Above, the electricity spot prices in DK1 are shown for the period 2020-01-01 to 2023-01-21. The red lines indicate the danish lockdown due to COVID-19, 2020-03-11, the Evergreen blockade of the Suez Canal, 2021-03-23, the Russian invasion of Ukraine, 2022-02-24 and the danish peak inflation of 10.1% in October 2022.

The data consists of three data sets. The first data set contains electricity spot prices per hour in DKK pr. MWh, over the period 2020-01-01 hour 0 (midnight), to 2023-01-31 hour 23 danish time. The price areas are Denmark east of the Great Belt (DK1), Denmark west of the great belt (DK2), Sweden southern part (SE4), Sweden area surrounding Stockholm (SE3), Norway southern part (NO2), Germany (DE).

The second data set contains data of electricity exchange between Denmark and neighboring countries and danish production of electricity in MWh per hour from fossile fuels (gas, oil, coal), sustainable energy (biomass, offshore wind, onshore wind, hydro) and incineration in the same time period.

The third data set contains data on consumption in kWh from different different grid companies per hour, in the period 2020-01-01 hour 0 (midnight), to 2023-01-21 hour 23.

This project is focusing exclusively on spot prices in the price area DK1, which is Denmark, east of the Great belt, and the project will analyze time series data of electricity spot prices, to find ARIMA/SARIMA models, which are better than a naive benchmark model, for forecasting these electricity spot prices.

# Data cleaning

In this section, we are referring to our work done in data_cleaning.qmd.

We have gathered our raw data from www.energidataservice.dk, where our data was in a .JSON file format.

After reading the data, we made sure that all our variables were read as the correct datatypes, and transformed all missing values into NA values. In the "Production" data set, we also dropped variables from the data set, that had too many missing values, to be useful for analysis. In the "Consumption" data set, we only had 3 variables, but still a lot of missing values. To keep as much information from the "Consumption" data set, we transformed all the missing values to 0's.

After having cleaned the columns, we then moved on to the rows. We found out that the start and end times of our 3 data sets did not match. All of the data sets had the same first observation regarding time, but the "Consumption" data set, was missing some observations at the end. Hence, we had to cut off data from the other two data sets, to make them all equal length.

We then had a strange problem, where there were 3 seemingly random observations, that were missing from the "Production" data set. We then had to use interpolation to add the previous observation's values to the missing rows, to fill in the missing observations.
We are aware that this modification of 3 out of 26808 observations can have an impact on the results, but we assume the impact to be rather insignificant, as well as the modification being a necessity for further analysis, due to the nature of time series, and for the models to work as intended.

At last, we also divided the columns in the "Consumption" data set with 1000, since we found out that the electricity used in the "Consumption" data set was in kwh, where we had electricity in Mwh in the other data sets.

Having the data in a .JSON file format made reading the data very slow, so we decided to save the cleaned data as a .csv file format, which made it much faster to read.

We then followed the same process to create cleaned test data, to measure our model's performance at the end of the report.

# Trend

In this section, we are referring to our work done in Detrending.Rmd.

The first thing we want to look at when dealing with time series data, is to check whether or not the time series is stationary. This is difficult to test, so instead you will be testing whether or not the time series data is covariance stationary, which can be done with an ADF (Augmented Dickey-Fuller) test, a KPSS (Kwiatkowski-Phillips-Schmidt-Shin) test or by observing the ACF (Auto-Correlation Function) plot.

The Dickey Fuller test for a simple AR(1) model:
$$
\begin{split}
  y_t &= \rho y_{t-1} + u_t\\
  \Delta y_t &= (\rho-1) y_{t-1} + u_t = \gamma y_{t-1} + u_t
\end{split}
$$
The null states that a unit root is present $\rho=1 \Rightarrow \gamma=0$, the alternative states that it's stationary.

The KPSS test, tests for stationarity around a trend without the presence of a unit root process.
$$
\begin{split}
  y_t &= \alpha + \beta t + z_t\\
  z_t &= r_t + u_t\\
  r_t &= r_{t-1} + v_t
\end{split}
$$
For time series $y_t$, residual $z_t$, short term memory $u_t$ and long term memory unit root $r_t$ where $r_0=0$ and $v_t$ i.i.d. with mean 0 and variance $\sigma^2_v$.

The null states that $y_t$ is trend stationary which implies $\sigma^2_v=0$, the alternative states that a unit root is present.

We first check if the raw data is covariance stationary.

```{r}
acf(prices$spot_price_dkk, lag.max = 1000)
```

```{r message=FALSE, warning=FALSE}
adf.test(prices$spot_price_dkk)
kpss.test(prices$spot_price_dkk)
```

We observe that the two tests don't agree. The null hypothesis for the ADF test is "Not stationary", while the null hypothesis for the KPSS test is "Stationary".
When observing the ACF plot, we can clearly see that there is high autocorrelation in the data. Hence, we trust the KPSS test here, and conclude that the time series is not covariance stationary.

Next step is then to try to transform the data, so that it becomes stationary. This is a necessity to assure the correctness of the models, we will be using later on (ARIMA/SARIMA).

There are multiple possible methods to try, when having to transform the data to become stationary.

One could be to try to take the logarithm on the data, but this does not work in our case, since we in some cases have negative values. We tried a logarithmic transformation where the log of negative values are replaced by 0, but this gave many outliers and structural breaks. This can be seen in the data_transformation.Rmd file.

A different also very common approach is to first difference the data, so that every observation is subtracted from the previous observation.

At last, an approach is to try to detrend the data, by fitting a regression to the data to mimic the underlying trend, which afterwards can be subtracted from the data, to remove the fitted trend.

We first try to remove the underlying trend with a linear regression.

```{r}
prices$t <- sequence(26808, 1)

# Function for calculating sum of squared error
sse <- function(data, par) {
  with(data, sum((trendfn(data, par) - spot_price_dkk)^2))
}
```

```{r message=FALSE, warning=FALSE}
trendfn <- function(data, par) {
  with(data, par[1] + par[2]*t)
}

optimum <- optim(c(1, 2), sse, data=prices)
par_result <- optimum[[1]]
optimum

prices_trend <- trendfn(prices, par_result)
residuals_trend <- prices$spot_price_dkk - prices_trend

plot(prices$spot_price_dkk, main="Trend function", col=alpha("black", 0.5), type="l")
lines(prices$t, prices_trend, col="red")

plot(prices$t, residuals_trend, main="Residuals/detrend", col=alpha("black", 0.5), type="l")
print(c("RMSE:", sqrt(mean(residuals_trend^2))))
```

We observe, that a linear trend looks to be good at catching the trend in the "stable" first part of the data, but gets worse at catching the trend in the "volatile" last part of the data.

```{r}
acf(residuals_trend, lag.max=1000)
```

We then evaluate the result from the detrending against the approach of first differencing using ADF test and KPSS test.

```{r message=FALSE, warning=FALSE}
print("Detrending")
adf.test(residuals_trend)
kpss.test(residuals_trend)
print("First Differencing")
adf.test(diff(prices$spot_price_dkk, 1))
kpss.test(diff(prices$spot_price_dkk, 1))
```

Looking at the KPSS test, the linear trend does not look to have made much of a difference regarding covariance stationarity, but the KPSS test says that the first differenced data is stationary.

We then tried different variants of sine functions, offset sine functions + linear trends and sums of offset since functions, to try to find a good fit of the data, where the best underlying function we found was the following sum of 7 offset sine functions.

```{r message=FALSE, warning=FALSE}
trendfn <- function(data, par) {
  with(data, par[1] 
       + par[2]*sin((t+par[3])/240) 
       + par[4]*sin((t+par[5])/480)
       + par[6]*sin((t+par[7])/720)
       + par[8]*sin((t+par[9])/960)
       + par[10]*sin((t+par[11])/1200)
       + par[12]*sin((t+par[13])/1440)
       + par[12]*sin((t+par[13])/1680))
}

subset_prices <- prices %>%
  filter(t >= 15000)
optimum <- optim(c(10000, 
                   5000, 0, 
                   2000, 24,
                   1000, 48,
                   100, 72,
                   100, 96,
                   100, 120,
                   1000, 144), sse, data=subset_prices)
par_result <- optimum[[1]]
optimum

prices_trend <- trendfn(subset_prices, par_result)
residuals_trend <- subset_prices$spot_price_dkk - prices_trend

plot(subset_prices$t, subset_prices$spot_price_dkk, main="Trend function", col=alpha("black", 0.5), type="l")
lines(subset_prices$t, prices_trend, col="red")

plot(subset_prices$t, residuals_trend, main="Residuals/detrend", col=alpha("black", 0.5), type="l")
print(c("RMSE:", sqrt(mean(residuals_trend^2))))
```

Note that we are here only trying to fit the sinusoidal regression on the "volatile" last part of the data. We then see that it looks like we have fitted the underlying function quite well, but we will have to check what the KPSS test says regarding stationarity.

```{r}
acf(residuals_trend, lag.max=1000)
```

```{r message=FALSE, warning=FALSE}
print("Detrending")
adf.test(residuals_trend)
kpss.test(residuals_trend)
print("First Differencing")
adf.test(diff(prices$spot_price_dkk, 1))
kpss.test(diff(prices$spot_price_dkk, 1))
```

Looking at the KPSS test, it now says that the time series is almost covariance stationary, but the ACF plot still does not look too good. It is also still significantly less covariance stationary, than just using first differencing.

We also tried using a nonparametric method to fit the underlying trend in the data, but this is of course not a viable solution, since it is nonparametric, and will of course not work for forecasting.

One thing, we did discover from the nonparametric model, was that when we gave it a span closer to 0, the detrending with the nonparametric model looked more and more similar to a first differencing.

```{r}
prices_loess <- loess(spot_price_dkk~t, data = prices, span=0.01)
prices_trend <- predict(prices_loess, prices$t)

plot(prices$t, prices$spot_price_dkk, main="Nonparametric fit", type='l')
lines(prices$t, prices_trend, col='red', lwd=2)

plot(prices$t, prices$spot_price_dkk - prices_trend, main="Residuals/detrend", type='l')

plot(diff(prices$spot_price_dkk, 1), main="First Differenced", type="l")
```

This made us decide on using first differencing to transform our data, to become covariance stationary.

# Seasonality

In this section, we are referring to our work done in Seasonality.Rmd.

Time series data often experiences some sort of seasonality caused by varying factors over specific time periods. To "catch" possible seasonality when creating models for forecasting, we want to check in what time periods the data experiences seasonality.

We start by checking for hourly and daily seasonality, by plotting the mean prices grouped by hour and weekday. We then see if we can find a recurring predictable pattern.

```{r message=FALSE, warning=FALSE}
prices %>%
  group_by(Hour, Wday) %>%
  summarize(hour=Hour, day=Wday, week=Week, month=Month, year=Year, mean_price=mean(spot_price_dkk)) %>%
  ggplot(aes(x=hour, 
             y=mean_price, 
             group=factor(day), 
             color=factor(day))) + 
  geom_line()
```

When looking at the plot, it is very clear that the prices are at their highest at around 6 AM and around 5 PM, as well as the prices being at their lowest at around 2 AM and around 12 PM. This pattern seems recurring and predictable. Hence, we note that the data experiences hourly seasonality.

For daily seasonality, it looks as if there is a recurring pattern of the prices being at their highest on mondays and tuesdays, and the prices being at their lowest on saturdays and sundays. We will check this over again, by now checking for daily and monthly seasonality, by plotting the mean prices grouped by weekday and month.

```{r message=FALSE, warning=FALSE}
prices %>%
  group_by(Wday, Month) %>%
  summarize(hour=Hour, day=Wday, week=Week, month=Month, year=Year, mean_price=mean(spot_price_dkk)) %>%
  ggplot(aes(x=day, 
             y=mean_price, 
             group=factor(Month), 
             color=factor(Month))) + 
  geom_line()
```

When looking at this plot, we again see the same pattern regarding daily seasonality. Hence, we will note that the data experiences daily seasonality.

For monthly seasonality, it looks like there is a recurring pattern of the prices being at their highest in the summer months, and the prices being at their lowest in the winter months, with the exception of December. We will again check this one more time, by now plotting the mean prices grouped by month and weekday.

```{r message=FALSE, warning=FALSE}
prices %>%
  group_by(Wday, Month) %>%
  summarize(hour=Hour, day=Wday, week=Week, month=Month, year=Year, mean_price=mean(spot_price_dkk)) %>%
  ggplot(aes(x=month, 
             y=mean_price, 
             group=factor(day), 
             color=factor(day))) + 
  geom_line()
```

This gives us a much clearer view of the monthly seasonality, and we can confirm what we saw on the previous plot. Hence, the data experiences monthly seasonality.

# Forecasting

In this section and in the section "Results", we are referring to our work done in Model_benchmark.Rmd.

When deciding on a method for forecasting, there is different ways to go about it. In this project, we have decided to forecast the next 24 hours (day-ahead forecasting), without reestimating the model. Day-ahead forecasting would in the real world be enough for a company to make a bid for the upcoming day, and the ARIMA/SARIMA model will also perform better, when it is not trying to forecast far into the future. We decided to not reestimate the model for each day we forecast, since this would then be a very slow process, but instead just keep the estimated coefficients, and give the model the data of the next 24 hours every time, to forecast the next day. We then evaluate the day-ahead forecast on validation data of the last 28 days, using the Root Mean Square Error (RMSE).

First of all, we needed to generate some naive benchmark models, to have some measures to evaluate our different model's performances against. We used a simple linear regression, a sinusoidal regression and the best fitted sum of multiple sinusoidal regressions we found from the section "Trends", to create some very naive benchmark models. We trained the models on the data without the last 28 days, and then used the last 28 days as validation data.

The results of these benchmark models can be seen in the Results table in the section "Results", where we could observe that the simple linear regression gave the best results.

We then created the function below to generate the day-ahead forecasts using different auto-regressive models such as the ARIMA/SARIMA. The SARIMA(p, d, q)(P, D, Q)(T) is defined as:

$$
\Delta_d y_t = \sum_{i=1}^{p} \beta_i \Delta_d y_{t-i} + \sum_{j=1}^{q} \gamma_j \Delta_d u_{t-j} + \sum_{k=1}^{P} \zeta_k \Delta_D y_{t-T\cdot k} + \sum_{l=1}^{Q} \Delta_D \eta_l u_{t-T\cdot l} + \Delta_d u_t
$$
For time series $y_t$, constants $\beta_i, \gamma_j, \zeta_k, \eta_l$ and residual $u_t$ with mean 0 and variance $\sigma^2$.

The ARIMA model is defined in the same way as the SARIMA model above, but without the seasonal component equal to the last two sums in the above equation. Furthermore, an intercept $\alpha$ is included in the model when $d=0$.

```{r}
forecast_func <- function(days_ahead, data,
                          fix_vec_AR, d, fix_vec_MA, 
                          fix_vec_SAR, D, fix_vec_SMA,
                          Tperiod,
                          plotbool,
                          set_coefs) {
  training_data <- data[1:(length(data)-24*days_ahead)]
  validation_data <- data[(length(training_data)+1):length(data)]
  subset_training_data <- data[(length(training_data)-24*7-50):length(training_data)]
  
  forecasts <- c()
  
  start <- Sys.time()
  
  AR_f_coef <- set_coefs
  if(is.null(set_coefs)) {
    AR_f <- arima(x=training_data, 
                  order=c(length(fix_vec_AR), d, length(fix_vec_MA)), 
                  fixed = c(fix_vec_AR, fix_vec_MA, fix_vec_SAR, fix_vec_SMA),
                  seasonal = list(order=c(length(fix_vec_SAR), D, length(fix_vec_SMA)),
                                  period=Tperiod),
                  include.mean = F,
                  method="CSS")
    AR_f_coef <- AR_f$coef
  }
  
  for (i in 1:days_ahead){
    AR_f <- arima(x=subset_training_data,
                  order=c(length(fix_vec_AR), d, length(fix_vec_MA)), 
                  fixed = AR_f_coef,
                  seasonal = list(order=c(length(fix_vec_SAR), D, length(fix_vec_SMA)),
                                  period=Tperiod),
                  include.mean = F,
                  method="CSS")
    forecasts <- c(forecasts, as.data.frame(forecast(AR_f, h=24))[,1])
    subset_training_data <- c(subset_training_data, validation_data[(24*(i-1)+1):(24*i)])
  }
  end <- Sys.time()
  print(end-start)
  residuals <- validation_data-forecasts
  RMSE <- sqrt(mean(residuals^2))
  
  print(c("RMSE:", RMSE))
  if(plotbool == T) {
    plot(training_data, type="l", 
         xlim=c(length(training_data)-250, length(data)+10), ylim=c(-500, 4500),
         main="Forecasts", xlab="Time", ylab="Price")
    lines(x=(length(training_data)+1):length(data) , y=forecasts, col="blue")
    lines(x=(length(training_data)+1):length(data) , y=validation_data, col=alpha("red", 0.5))
    legend("topright", inset=0.02, legend=c("Forecast", "Data"), col=c("blue", "red"), lty=1)
  }
  return(list(forecasts, residuals, RMSE, AR_f_coef))
}
```

The forecasting function takes inputs of the days ahead, data of the time series, hyperparameters p, d, q, P, D, Q, T including which are fixed and free, whether or not the forecast should be plotted and the possibility to include a set of predetermined coefficients.

Firstly, the data is split into training data, which consists of all data except the last $24\cdot \text{days ahead}$ observations, which is the validation data. If the coefficients are not predetermined, then an ARIMA model is fitted to the training data, to obtain coefficients.

As far as we are aware, no R libraries support day-ahead forecasting for multiple days, so our approach is to use a fixed window, meaning that the estimated ARIMA coefficients are kept constant during forecasting. Using training data, we forecast only 24 hours into the future. Afterwards, the 24 hour validation data is added to the training data. This process is repeated for the number of days to forecast. One issue with this forecasting approach is that, even though the coefficients are fixed, calling the ARIMA object using all training data multiple times, takes a very long time. Instead, we only use a subset of the most recent training data, which significantly speeds up the forecasting process, without impacting the forecast.

We ran this function on many different models, where we chose the lags based on what we found in the seasonality analysis. Hence, some different combinations of 24 hour lags and seasonal 24 hour lags for each day up until one week back, equal to 168 hour lags for a seasonal lag of 7.

We tried to run some of the models, where we fixed many of the coefficients at zero. For example, when looking at the Results table in the section "Results", there is a column called "free_arima". If that column value says "24th", then it means that we fixed the 23 lags up until the 24th lag to 0, such that the current price only is dependent on the price 24 hours ago. The same applies for the "free_seasonal" column, and if these column values are empty, it means that none of the coefficients have been fixed at zero.

Below is an example on how we ran the function, on what we found to be the best model without exogenous variables.

## SARIMA(24, 1, 1)(7, 0, 1)(24)
```{r}
outputlist <- forecast_func(28, prices$spot_price_dkk,  
                            c(rep(NA, 24)), 1, c(NA), 
                            c(rep(NA, 7)), 0, c(NA),
                            24,
                            plotbool=T,
                            set_coefs=NULL)
forecasts <- outputlist[[1]]
residuals <- outputlist[[2]]
RMSE <- outputlist[[3]]
AR_f_coef <- outputlist[[4]]
print(AR_f_coef)

hist(residuals, breaks=50)
qqnorm(residuals)
qqline(residuals)
```

After having found the best model for forecasting without exogenous variables, we moved on to add exogenous variables to the models, to try to better describe the prices with correlated data of the electricity production.

```{r}
forecast_func_xreg <- function(days_ahead, data, xogen,
                          fix_vec_AR, d, fix_vec_MA, 
                          fix_vec_SAR, D, fix_vec_SMA,
                          Tperiod,
                          plotbool,
                          set_coefs) {
  training_data <- data[1:(length(data)-24*days_ahead)]
  validation_data <- data[(length(training_data)+1):length(data)]
  subset_training_data <- data[(length(training_data)-24*7-50):length(training_data)]
  
  xogen_training <- xogen[1:length(training_data), ]
  xogen_validation <- xogen[(length(training_data)+1):length(data), ]
  xogen_subset <- xogen[(length(training_data)-24*7-50):length(training_data), ]
  
  forecasts <- c()
  
  start <- Sys.time()
  
  AR_f_coef <- set_coefs
  
  fix_vec_xreg <- rep(NA, ncol(xogen))
  if(is.null(set_coefs)) {
    model <- arima(x=training_data, 
                  xreg=matrix(xogen_training, ncol=ncol(xogen), byrow=F),
                  order=c(length(fix_vec_AR), d, length(fix_vec_MA)), 
                  fixed = c(fix_vec_AR, fix_vec_MA, fix_vec_SAR, fix_vec_SMA, fix_vec_xreg),
                  seasonal = list(order=c(length(fix_vec_SAR), D, length(fix_vec_SMA)),
                                  period=Tperiod),
                  include.mean = F,
                  method="CSS")
    
    AR_f <- Arima(y=training_data,
                  model=model,
                  xreg=matrix(xogen_training, ncol=ncol(xogen), byrow=F),
                  include.mean=F)
    
    AR_f_coef <- AR_f$coef
  }
  
  for (i in 1:days_ahead){
    model <- arima(x=subset_training_data,
                  xreg=matrix(xogen_subset, ncol=ncol(xogen), byrow=F),
                  order=c(length(fix_vec_AR), d, length(fix_vec_MA)), 
                  fixed=AR_f_coef,
                  seasonal=list(order=c(length(fix_vec_SAR), D, length(fix_vec_SMA)),
                                  period=Tperiod),
                  include.mean=F,
                  method="CSS")
    
    AR_f <- Arima(y=subset_training_data,
                  model=model,
                  xreg=matrix(xogen_subset, ncol=ncol(xogen), byrow=F),
                  include.mean = F)
    
    forecasts <- c(forecasts, 
                   as.data.frame(forecast(AR_f,
                                          xreg=matrix(xogen_validation[(24*(i-1)+1):(24*i), ],
                                                      ncol=ncol(xogen), byrow=F)))[,1])
    
    xogen_subset <- rbind(xogen_subset, xogen_validation[(24*(i-1)+1):(24*i), ])
    subset_training_data <- c(subset_training_data, validation_data[(24*(i-1)+1):(24*i)])
    
  }
  end <- Sys.time()
  print(end-start)
  residuals <- validation_data-forecasts
  RMSE <- sqrt(mean(residuals^2))
  
  print(c("RMSE:", RMSE))
  if(plotbool == T) {
    plot(training_data, type="l", 
         xlim=c(length(training_data)-250, length(data)+10), ylim=c(-500, 4500),
         main="Forecasts", xlab="Time", ylab="Price")
    lines(x=(length(training_data)+1):length(data) , y=forecasts, col="blue")
    lines(x=(length(training_data)+1):length(data) , y=validation_data, col=alpha("red", 0.5))
    legend("topright", inset=0.02, legend=c("Forecast", "Data"), col=c("blue", "red"), lty=1)
  }
  return(list(forecasts, residuals, RMSE, AR_f_coef))
}
```

The basic structure is equivalent to the previous function, but the exogenous variables are also split into training data and validation data. We encountered a strange problem, where fitting the arima()-function with exogenous variables works, but forecasting with the forecast()-function is not supported. We found a workaround by using the Arima()-function as a wrapper. The issue is likely caused by the arima()-function originating from the "stats" library, where the Arima()-function and forecast()-function are from the "forecast" library.

We evaluated many different combinations of exogenous variables, to see which exogenous variables had a positive impact on describing the data, and which exogenous variables didn't have a positive impact on describing the data, or even lead to possible overfitting.

It is worth noting that we didn't evaluate the different combinations of exogenous variables on the best model we found without exogenous variables, but instead a simpler ARIMA(24,1,0) model, which also performed well. This was a choice we made, since the SARIMA(24,1,1)(7,0,1) took a very long time to fit, so we found the best combinations of exogenous variables for the ARIMA(24,1,0) model, and then tried to use the same exogenous variables on the SARIMA(24,1,1)(7,0,1). We then observed that SARIMA(24, 1, 1)(7, 0, 1) with these exogenous variables, was the best model for forecasting with exogenous variables as well.

Below is an example on how we ran the function, on what we found to be the best model with exogenous variables.

## ARIMA(24, 1, 1)(7, 0, 1)(24) + weekend + wind + fossile + biomass + waste + exchange
```{r}
xregs <- cbind(prices$Sat, prices$Sun, 
              prod$fossil_gas, prod$fossil_oil, prod$fossil_hard_coal,
              prod$onshore_wind_power, prod$offshore_wind_power,
              prod$biomass, prod$waste,
              prod$exchange_great_belt, prod$exchange_nordic_countries, prod$exchange_continent)

outputlist <- forecast_func_xreg(28, prices$spot_price_dkk, xregs, 
                            c(rep(NA, 24)), 1, c(NA), 
                            c(rep(NA, 7)), 0, c(NA),
                            24,
                            plotbool=T,
                            set_coefs=NULL)
forecasts <- outputlist[[1]]
residuals <- outputlist[[2]]
RMSE <- outputlist[[3]]
AR_f_coef <- outputlist[[4]]
print(AR_f_coef)

hist(residuals, breaks=50)
qqnorm(residuals)
qqline(residuals)
```

Hence, we found that the best model from this test on the last 28 days as validation data, was the SARIMA(24,1,1)(7,0,1) model with exogenous variables, and the best simpler model with exogenous variables was the ARIMA(24,1,0) model.

To check that the models were not overfitted to the chosen last 28 days of data, we also used cross-validation. To do this with time series data, we split the data into 4 different subsets, which we chose to be 3 evenly split subsets of the volatile part of the data, and a 4th subset of all the volatile part of the data.

The results of this cross-validation can be seen in the Cross-validation table in the section "Results", where we observed that SARIMA(24,1,1)(7,0,1) model didn't seem to be overfitted, when compared to the simpler ARIMA(24,1,0) model.

# Results

Below, is the results of the different models that have been evaluated. In the Results table, the models have been trained on all the training data except the last 28 days, that have been used as validation data.

In the Cross-validation table, are the results from the models, we ran through cross-validation.

## Results table

```{r include=FALSE}
Result_table <- data.frame(model = character(), free_arima = character(), seasonal = character(), free_seasonal = character(), xreg = character(), RMSE = double(), stringsAsFactors = FALSE)
```

```{r echo=FALSE}
Result_table <- rbind(Result_table, list("Linear Regression", "", "", "", "", 499.7))
Result_table <- rbind(Result_table, list("Sinusoidal Regression", "", "", "", "", 847.4))
Result_table <- rbind(Result_table, list("Sum of Sinusoidal Regressions", "", "", "", "", 1535.0))
Result_table <- rbind(Result_table, list("ARIMA(1,1,0)", "", "", "", "", 431.5))
Result_table <- rbind(Result_table, list("ARIMA(24,1,0)", "24th", "", "", "", 417.5))
Result_table <- rbind(Result_table, list("ARIMA(24,1,0)", "", "", "", "", 371.8))
Result_table <- rbind(Result_table, list("SARIMA(24,1,0)", "24th", "(7,0,0)", "1st, 2nd, 7th", "", 380.9))
Result_table <- rbind(Result_table, list("SARIMA(24,1,0)", "", "(7,0,0)", "", "", 362.7))
Result_table <- rbind(Result_table, list("SARIMA(1,1,0)", "", "(7,0,0)", "1st, 2nd, 7th", "", 388.8))
Result_table <- rbind(Result_table, list("SARIMA(1,1,0)", "", "(7,0,0)", "", "", 368.0))
Result_table <- rbind(Result_table, list("SARIMA(24,1,0)", "", "(7,1,0)", "", "", 363.7))
Result_table <- rbind(Result_table, list("SARIMA(24,1,1)", "", "(7,0,0)", "", "", 369.2))
Result_table <- rbind(Result_table, list("SARIMA(24,1,1)", "24th", "(7,0,0)", "", "", 362.5))
Result_table <- rbind(Result_table, list("SARIMA(24,1,1)", "24th", "(7,0,0)", "1st, 2nd, 7th", "", 381.2))
Result_table <- rbind(Result_table, list("SARIMA(24,1,1)", "", "(7,0,1)", "", "", 344.0))
Result_table <- rbind(Result_table, list("SARIMA(24,1,1)", "24th", "(7,0,1)", "", "", 362.0))
Result_table <- rbind(Result_table, list("SARIMA(24,1,1)", "", "(7,0,1)", "1st, 2nd, 7th", "", 356.7))
Result_table <- rbind(Result_table, list("SARIMA(24,1,1)", "24th", "(7,0,1)", "1st, 2nd, 7th", "", 356.7))
Result_table <- rbind(Result_table, list("SARIMA(24,1,24)", "", "(7,0,0)", "", "", 346.6))
Result_table <- rbind(Result_table, list("SARIMA(24,1,24)", "", "(7,0,7)", "", "", 361.2))
Result_table <- rbind(Result_table, list("ARIMA(24,1,0)", "", "", "", "Weekend", 366.3))
Result_table <- rbind(Result_table, list("SARIMA(24,1,1)", "", "(7,0,1)", "", "Weekend", 341.2))
Result_table <- rbind(Result_table, list("ARIMA(24,1,0)", "", "", "", "Weekdays", 365.0))
Result_table <- rbind(Result_table, list("ARIMA(24,1,0)", "", "", "", "Weekend, Season", 366.4))
Result_table <- rbind(Result_table, list("ARIMA(24,1,0)", "", "", "", "Weekend, Wind", 316.8))
Result_table <- rbind(Result_table, list("ARIMA(24,1,0)", "", "", "", "Weekend, Wind, Solar", 318.6))
Result_table <- rbind(Result_table, list("ARIMA(24,1,0)", "", "", "", "Weekend, FossilProduce", 347.7))
Result_table <- rbind(Result_table, list("ARIMA(24,1,0)", "", "", "", "Weekend, Wind, FossilProduce", 305.8))
Result_table <- rbind(Result_table, list("ARIMA(24,1,0)", "", "", "", "Weekend, Wind, FossilProduce, Consumption", 305.2))
Result_table <- rbind(Result_table, list("ARIMA(24,1,0)", "", "", "", "Weekend, Wind, FossilProduce, Biomass", 302.4))
Result_table <- rbind(Result_table, list("ARIMA(24,1,0)", "", "", "", "Weekend, Wind, FossilProduce, Biomass, Waste", 299.4))
Result_table <- rbind(Result_table, list("ARIMA(24,1,0)", "", "", "", "Weekend, Wind, FossilProduce, Biomass, Waste, Exchange", 276.1))
Result_table <- rbind(Result_table, list("ARIMA(1,1,0)", "", "", "", "Weekend, Wind, FossilProduce, Biomass, Waste, Exchange", 303.5))
Result_table <- rbind(Result_table, list("SARIMA(24,1,0)", "24th", "(7,0,0)", "1st, 2nd, 7th", "Weekend, Wind, FossilProduce, Biomass, Waste, Exchange", 311.8))
Result_table <- rbind(Result_table, list("SARIMA(24,1,1)", "", "(7,0,1)", "", "Weekend, Wind, FossilProduce, Biomass, Waste, Exchange", 263.9))
colnames(Result_table) <- c("model", "free_arima", "seasonal", "free_seasonal", "xreg", "RMSE")
kbl(Result_table) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "bordered"), full_width = F, position = "left", font_size = 10)
```

## Cross-validation table

```{r include=FALSE}
CV_table <- data.frame(model = character(), free_arima = character(), seasonal = character(), free_seasonal = character(), xreg = character(), RMSE_subset_1 = double(), RMSE_subset_2 = double(), RMSE_subset_3 = double(), RMSE_subset_4 = double(), mean_RMSE = double(), sd_RMSE = double(), stringsAsFactors = FALSE)
```

```{r echo=FALSE}
CV_table <- rbind(CV_table, list("ARIMA(24,1,0)", "", "", "", "", 441.3, 704.8, 442.1, 371.0, 489.8, 147.2))
CV_table <- rbind(CV_table, list("SARIMA(24,1,1)", "", "(7,0,1)", "", "", 393.3, 661.9, 391.7, 342.1, 447.3, 145.1))
CV_table <- rbind(CV_table, list("ARIMA(24,1,0)", "", "", "", "Weekend, Wind", 354.6, 674.8, 405.2, 364.3, 449.7, 151.6))
CV_table <- rbind(CV_table, list("ARIMA(24,1,0)", "", "", "", "Weekend, Wind, FossilProduce, Biomass, Waste, Exchange", 240.9, 543.2, 328.5, 364.3, 369.2, 127.1))
CV_table <- rbind(CV_table, list("SARIMA(24,1,1)", "", "(7,0,1)", "", "Weekend, Wind, FossilProduce, Biomass, Waste, Exchange", 233.0, 539.5, 316.7, 337.7, 356.7, 130.0))
CV_table <- rbind(CV_table, list("auto.arima(23,1,0)", "", "", "", "Weekend, Wind, FossilProduce, Biomass, Waste, Exchange", 240.5, 533.7, 345.2, 359.4, 369.7, 121.5))
colnames(CV_table) <- c("model", "free_arima", "seasonal", "free_seasonal", "xreg", "sub_RMSE_1", "sub_RMSE_2", "sub_RMSE_3", "sub_RMSE_4", "mean_RMSE", "sd_RMSE")
kbl(CV_table) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "bordered"), full_width = F, position = "left", font_size = 10)
```

## auto.arima()

Before we ran the best SARIMA(24,1,1)(7,0,1) model on the test data, we wanted to test our model, against a model that would be suggested by the auto.arima()-function from the library "forecast".

The auto.arima()-function came with very different suggestions of models, depending on what initial guess was given to the model. Hence, we gave it the initial guess of the best SARIMA(24,1,1)(7,0,1) model as it's initial guess, which produced a suggested ARIMA(23,1,0) model for both with and without exogenous variables.

We then tried to use this model for day-ahead forecasting and saw that it was a lot worse than the SARIMA(24,1,1)(7,0,1) model without exogenous variables, but it was also on pair with the SARIMA(24,1,1)(7,0,1) model when exogenous variables were included.

We then tried running it through cross-validation, to see if the ARIMA(23,1,0) model suggested by the auto.arima()-function, might have been overfitted to the chosen last 28 days of the data as validation data. The results from cross-validation on the auto.arima() model, can also be seen in the Cross-validation table.

As might have been expected, it looks like the ARIMA(23,1,0) model, suggested by the auto.arima()-function, was lucky to have performed very well on the last 28 days of the data, and generally looks to be worse than the SARIMA(24,1,1)(7,0,1) model, and on pair with the very similar ARIMA(24,1,0) model. Hence, we will conclude that the best performing model from the cross-validation and model validation, was the SARIMA(24,1,1)(7,0,1) model, that we at last test on new test data.

## Test data
```{r include=FALSE}
test_prices <- read.csv("data/test_price_clean.csv") %>%
  mutate(
    hour_utc = lubridate::as_datetime(hour_utc),
    hour_dk = lubridate::as_datetime(hour_dk),
    spot_price_dkk = as.numeric(spot_price_dkk))
test_con <- read.csv("data/test_con_clean.csv")
test_prod <- read.csv("data/test_prod_clean.csv")
```

```{r include=FALSE}
test_prices <- test_prices %>%
  arrange(hour_utc) %>%
  filter(price_area == "DK1")

test_con <- test_con %>%
  arrange(hour_utc)

test_prod <- test_prod %>%
  filter(price_area == "DK1") %>%
  arrange(hour_utc)
```

```{r include=FALSE}
test_prices["t"] <- length(prices$spot_price_dkk) + 1:length(test_prices$spot_price_dkk)
test_prices["Hour"] <- hour(test_prices$hour_utc)
test_prices["Wday"] <- wday(test_prices$hour_utc, label=T)
test_prices["Week"] <- week(test_prices$hour_utc)
test_prices["Month"] <- month(test_prices$hour_utc)
test_prices["Year"] <- year(test_prices$hour_utc)

test_prices["Sun"] <- ifelse(test_prices$Wday=="Sun", 1, 0)
test_prices["Sat"] <- ifelse(test_prices$Wday=="Sat", 1, 0)
test_prices["Fri"] <- ifelse(test_prices$Wday=="Fri", 1, 0)
test_prices["Thu"] <- ifelse(test_prices$Wday=="Thu", 1, 0)
test_prices["Wed"] <- ifelse(test_prices$Wday=="Wed", 1, 0)
test_prices["Tue"] <- ifelse(test_prices$Wday=="Tue", 1, 0)
test_prices["Mon"] <- ifelse(test_prices$Wday=="Mon", 1, 0)

test_prices["Cutoff"] <- ifelse(test_prices$t >= 15000, 1, 0)

test_prices["Winter"] <- ifelse(test_prices$Month %in% c(12, 1, 2), 1, 0)
test_prices["Spring"] <- ifelse(test_prices$Month %in% c(3, 4, 5), 1, 0)
test_prices["Summer"] <- ifelse(test_prices$Month %in% c(6, 7, 8), 1, 0)
test_prices["Autumn"] <- ifelse(test_prices$Month %in% c(9, 10, 11), 1, 0)
```

### ARIMA(24, 1, 1)(7, 0, 1)(24) + weekend + wind + fossile + biomass + waste + exchange on 28 days of test data

```{r}
full_prices <- rbind(prices, test_prices[1:(24*28), ])
full_prod <- rbind(prod, test_prod[1:(24*28), ])

xregs <- cbind(full_prices$Sat, full_prices$Sun, 
              full_prod$fossil_gas, full_prod$fossil_oil, full_prod$fossil_hard_coal,
              full_prod$onshore_wind_power, full_prod$offshore_wind_power,
              full_prod$biomass, full_prod$waste,
              full_prod$exchange_great_belt, full_prod$exchange_nordic_countries, full_prod$exchange_continent)

outputlist <- forecast_func_xreg(28, full_prices$spot_price_dkk, xregs, 
                            c(rep(NA, 24)), 1, c(NA), 
                            c(rep(NA, 7)), 0, c(NA),
                            24,
                            plotbool=T,
                            set_coefs=NULL)
forecasts <- outputlist[[1]]
residuals <- outputlist[[2]]
RMSE <- outputlist[[3]]
AR_f_coef <- outputlist[[4]]
print(AR_f_coef)

hist(residuals, breaks=50)
qqnorm(residuals)
qqline(residuals)
```

### ARIMA(24, 1, 1)(7, 0, 1)(24) + weekend + wind + fossile + biomass + waste + exchange on 59 days of test data

```{r}
full_prices <- rbind(prices, test_prices[1:(24*59), ])
full_prod <- rbind(prod, test_prod[1:(24*59), ])

xregs <- cbind(full_prices$Sat, full_prices$Sun, 
              full_prod$fossil_gas, full_prod$fossil_oil, full_prod$fossil_hard_coal,
              full_prod$onshore_wind_power, full_prod$offshore_wind_power,
              full_prod$biomass, full_prod$waste,
              full_prod$exchange_great_belt, full_prod$exchange_nordic_countries, full_prod$exchange_continent)

outputlist <- forecast_func_xreg(59, full_prices$spot_price_dkk, xregs, 
                            c(rep(NA, 24)), 1, c(NA), 
                            c(rep(NA, 7)), 0, c(NA),
                            24,
                            plotbool=T,
                            set_coefs=NULL)
forecasts <- outputlist[[1]]
residuals <- outputlist[[2]]
RMSE <- outputlist[[3]]
AR_f_coef <- outputlist[[4]]
print(AR_f_coef)

hist(residuals, breaks=50)
qqnorm(residuals)
qqline(residuals)
```

We observe that the SARIMA(24,1,1)(7,0,1) model performs exceptionally well on the new test data.
